---
title: "Machine Learning-R"
author: "Thomas Laurent"
date: "29/08/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
myVariableNames=c("Comptes","Duree_credit","Historique_credit","Objet_credit","Montant_credit","Epargne","Anciennete_emploi","Taux_effort","Situation_familiale","Garanties","Anciennete_domicile","Biens","Age","Autres_credits","Statut_domicile","Nb_credits","Type_emploi","Nb_pers_charge","Telephone","Etranger","Cible")
# 
credit=read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data",h=FALSE,col.names=myVariableNames)

#credit=read.table("http://blogperso.univ-rennes1.fr/stephane.tuffery/publ#ic/german.txt",h=FALSE,col.names=myVariableNames)

#OId des observations
credit$Cle=seq(1,nrow(credit))
credit$Etranger=NULL
```

```{r}
#Variable transformation
credit$Cible[credit$Cible==1]=0
credit$Cible[credit$Cible==2]=1

credit$Cible=factor(credit$Cible)

varquali=c("Comptes","Epargne","Historique_credit","Objet_credit","Situation_familiale","Garanties","Biens","Autres_credits","Statut_domicile","Type_emploi","Anciennete_emploi","Telephone","Nb_pers_charge")

for (v in varquali) {credit[[v]]=factor(credit[[v]])}

varquanti=c("Duree_credit","Montant_credit","Taux_effort","Anciennete_domicile","Age","Nb_credits")

vars=-grep('Cle',names(credit))
```

```{r}
grep("Nb",names(credit))
```

```{r}
#Echantillonage
s=sort(sample(nrow(credit),nrow(credit)*2/3,replace=F))

table(credit[s,]$Cible)
```

#Exploration des donnees

```{r,fig.height=12,fig.width=12}
summary(credit[,vars])

library(Hmisc)
hist.data.frame(credit[,1:16])
```

```{r}
library(lattice)
histogram(~Duree_credit | Cible,data=credit,type="percent",col="grey",
          breaks=10)
histogram(~Montant_credit | Cible,data=credit,type="percent",col="grey",
          breaks=10)
histogram(~Age | Cible,data=credit,type="percent",col="grey",
          breaks=10)

by(credit[c("Age","Duree_credit","Montant_credit")],list(Cible=credit$Cible),
   summary)
```

```{r}
#Test de kruskal-wallis
sqrt(kruskal.test(credit$Age~credit$Cible)$statistic/sum(!is.na(credit$Age)))

#Discretisation
q=quantile(credit$Age,seq(0,1,by=0.1))
q[1]=q[1]-1
qage=cut(credit$Age,q)
tab=table(qage,credit$Cible)
prop.table(tab,1)

barplot(prop.table(tab,1)[,2],ylim=c(0,0.5),las=3,
        main="Age",ylab="Taux impayes",density=0)

#Age
Age=cut(credit$Age,c(0,25,Inf))
tab=table(Age,credit$Cible)
q=unique(quantile(credit$Duree_credit,seq(0,1,by=0.05)))

#Duree du credit
Duree_credit=cut(credit$Duree_credit,c(0,15,36,Inf))
tab=table(Duree_credit,credit$Cible)
prop.table(tab,1)

#Montant du credit
Montant_credit=cut(credit$Montant_credit,c(0,4000,Inf))
tab=table(Montant_credit,credit$Cible)
prop.table(tab,1)
```

```{r}
#Liaison des variables explocatives avec variable a expliquer
npred=-grep('(Cle|Cible|Duree_credit|Montant_credit|Age)',names(credit))

credit2=credit[,npred]
credit2$Age=Age
credit2$Duree_credit=Duree_credit
credit2$Montant_credit=Montant_credit

cramer=matrix(NA,ncol(credit2),3)
for (i in (1:ncol(credit2))){
  cramer[i,1]=names(credit2[i])
  cramer[i,2]=sqrt(chisq.test(table(credit2[,i],credit$Cible))$statistic/(length(credit2[,i])))
  cramer[i,3]=chisq.test(credit2[,i],credit$Cible)$p.value
}
colnames(cramer)=c("variable","V de Cramer","p-value chi-2")
vcramer=cramer[order(cramer[,2],decreasing=T),]

par(mar=c(8,4,4,0))
barplot(as.numeric(vcramer[,2]),
        col=gray(0:nrow(vcramer)/nrow(vcramer)),
        names.arg=vcramer[,1],ylab='V de Cramer',
        ylim=c(0,0.35),cex.names=0.8,las=3)
```

```{r}
#Recodage
ct=function(x){
  cat("\n",names(credit)[x],"\n")
  cbind(prop.table(table(credit[,x],credit$Cible),1),
        table(credit[,x]))
}

for (i in (1:ncol(credit))){
  if (!(names(credit[i])) %in% c("Cle","Cible","Duree_credit","Montant_credit",
                                 "Age"))
  {
    print(ct(i))
  }
}

library(car)
credit2$Comptes=recode(credit2$Comptes,"'A14'='Pas de compte';
                       'A11'='CC < 0 euros';'A12'='CC [0-200 euros[';
                       'A13'='CC > 200 euros'")

credit2$Historique_credit=recode(credit2$Historique_credit,
                                 "'A30'='Impayes passes';
                                 'A31'='Impaye en cours dans autre banque';c('A32','A33')='Pas de credits ou en cours sans retard';
                                 'A34'='Credits passes sans retard'")

credit2$Objet_credit=recode(credit2$Objet_credit,
                            "'A40'='Voiture neuve';'A41'='Voiture occasion';c('A42','A44','A45')='Interieur';'A43'='Video - HIFI';
                            c('A46','A48')='Etudes';
                            'A47'='Vacances';'A49'='Business';
                            else='Autres'")

credit2$Epargne=recode(credit2$Epargne,"
                       'A65'='Sans epargne';c('A61','A62')='< 500 euros';
                       c('A63','A64')='> 500 euros'")

credit2$Anciennete_emploi=recode(credit2$Anciennete_emploi,
                                 "c('A71','A72')='Sans emploi ou < 1 an';
                                 'A73'='entre 1 et 4 ans';
                                 c('A74','A75')='depuis au moins 4 ans'")

credit2$Situation_familiale=recode(credit2$Situation_familiale,
                                   "'A91'='Homme divorce/separe';
                                   'A92'='Femme divorcee/separee/mariee';
                                   c('A93','A94')='Homme celibataire/marie/veuf';'A95'='Femme celibataire'")

credit2$Garanties=recode(credit2$Garanties,
                         "'A103'='Avec garant';
                         else='Sans garant'")

credit2$Biens=recode(credit2$Biens,
                     "'A121'='Immobilier';
                     'A124'='Aucun bien';else='Non immobilier'")

credit2$Autres_credits=recode(credit2$Autres_credits,"'A143'='Aucun credit exterieur';
                              else='Credit exterieurs'")

credit2$Statut_domicile=recode(credit2$Statut_domicile,"'A152'='Proprietaire';
                              else='Non proprietaire'")
```

```{r}
#Tableau croise
library(gmodels)
CrossTable(credit$Comptes,credit$Cible,prop.chisq=F,chisq=TRUE,format="SAS")

summary(credit2)
```

```{r}
#Echantillon d'apprentissage et de test
set.seed(123)
id=(1:1000)[which(runif(1000)<0.66)]
train=credit2[id,]
test=credit2[-id,]
```

```{r}
#Liaison entre les variables explicatives
library(questionr)
cramer=matrix(NA,ncol(credit2),ncol(credit2))
for (i in (1:ncol(credit2))){
  for (j in (1:ncol(credit2)))
  {
    cramer[i,j]=cramer.v(table(credit2[,i],credit2[,j]))
  }
}

colnames(cramer)=colnames(credit2)
rownames(cramer)=colnames(credit2)

library(corrplot)
old=par(no.readonly = TRUE)
par(omi=c(0.4,0.4,0.4,0.4))
corrplot(cramer,type="upper",tl.srt=45,tl.col="black",diag=F,addCoef.col="black",
         addCoefasPercent=T)
par(old)
```

#Discretisation automatique

```{r}
library(pROC)
#Creation de la fonction decoup
decoup=function(base,x,y,h=0,k=0,pAUC=0,nbmod=3,calcul=1,
                algo="Nelder-Mead",graphe=0){
  #Renommage des variables
  attach(base)
  Xt=x
  Yt=y
  detach(base)
  
  #Traitement specifique des valeurs manquantes
  X=Xt[is.na(Xt)==0]
  Y=Yt[is.na(Xt)==0]
  
  seuils=as.vector(quantile(X,seq(0,1,length=(nbmod+1))))[2:nbmod]
  
  fitauc=function(s){
    s2=c(-Inf,unique(s),Inf)
    qX=cut(X,s2)
    tab=table(qX,Y)
    logit=glm(Y~qX,family=binomial(link="logit"))
    qXn=predict(logit,newdata=base[is.na(Xt)==0,],type="response")
    resultat=auc(Y,qXn,partial.auc=c(1,pAUC),partial.auc.correct=FALSE)*
      (1-sum((table(qX)/length(X))^2))/(1-(1-h)*(sum((table(qX)/length(X))^2)))*
      ((1-(1-k)*sum((prop.table(tab,1)[,2])^2)))/(1-sum((prop.table(tab,1)[,2])^2))
    return(-resultat)
  }
  
  #Application du decoupage
  Applical=function(){
    sf=unique(c(-Inf,est$par,Inf))
    qX=cut(Xt,sf)
    tab=table(qX,Yt,useNA="ifany")
    cat("\n","Resultat du decoupage :","\n")
    cat("\n","Seuils   %Negat.  %  Posit. #  +  #    %  #","\n")
    print(cbind(prop.table(tab,1)*100,tab[,2],table(qX,useNA="ifany"),table(qX,useNA="ifany")*100/length(Xt)))
    cat("\n","Indicateur de convergence (0=convergence optimisation)","\n")
    cat(est$convergence) #verifier qu'on obtient 0
    cat("\n","AUC (partielle) maximisee :","\n")
    cat(-est$value)
    cat("\n","Homogeneite des classes (0 <- faible ...  forte-> 1) :","\n")
    cat(1-sum((table(addNA(qX))/length(Xt))^2))
    return(qX)
  }
  
  #Calcul aire sous la courbe ROC
  Gini=function(t){
    cat("\n","AUC avant decoupage","\n")
    logit=glm(Y~X,family=binomial(link="logit"))
    g1=auc(Y,predict(logit,newdata=base[is.na(Xt)==0,],type="response"))
    cat(g1)
    cat("\n","AUC apres decoupage :","\n")
    logit=glm(Yt~t,family=binomial(link="logit"))
    g2=auc(Yt,predict(logit,newdata=base,type="response"))
    cat(g2)
    cat("\n","% Evolution AUC avant/apres decoupage :","\n")
    cat(100*(g2-g1)/g1)
    cat("\n")
  }
  
  #recherche des seuils optimaux a partir des valeurs initiales precedentes calcul=1
  #ou utilisation de bornes prescrites (calcul=0)
  if (calcul==1) {est=optim(seuils,fitauc,method=algo)}
  else {est=list() ; est$par=bornes ; est$convergence=0; est$value=0}
  
  cat("\n","____________________________________________________","\n")
  cat("\n","Discretisation de",substitute(x),"en",nbmod,"classes (algorithme ",algo,") \n")
  cat("\n","____________________________________________________","\n") 
  
  qX1=Applical()
  Gini(qX1)
  
  #courbe de densite
  if (graphe==1){
    layout(matrix(c(1,2)),heights=c(3,1))
    par(mar=c(2,3,2,1))
    base0=X[Y==0]
    base1=X[Y==1]
    xlim1=range(c(base0,base1))
    ylim1=c(0,max(max(density(base0)$y),max(density(base1)$y)))
    plot(density(base0),main=" ",col="blue",ylab=paste("Densite de ",
                                                       deparse(substitute(x))),
         xlim=xlim1,ylim=ylim1,lwd=2)
    lines(density(base1),col="red",lty=3,lwd=2)
    legend("topright",c(paste(deparse(substitute(y)),"=0"),
                        paste(deparse(substitute(y))," =1")),
           lty=c(1,3),col=c("blue","red"),lwd=2)
    abline(v=est$par,lty=2)
    texte=c("Chi de Kruskal-Wallis = \n \n",
            round(kruskal.test(X~Y)$statistic,digits=3))
    text(xlim1[2]*0.8,ylim1[2]*0.5,texte,cex=0.75)
    plot(X~Y,horizontal=TRUE,xlab=deparse(substitute(y)),
         col=c("blue","red"))
  }
  
  #fin de la fonction de discretisation automatique
  
}

rm(Age)
rm(Duree_credit)
rm(Montant_credit)

decoup(credit,Duree_credit,Cible,nbmod=3,h=1,k=0,pAUC=0.8,algo="BFGS",graphe=1)
```

```{r}
for (i in (2:5)) {
  decoup(credit,Age,Cible,nbmod=i,h=0,k=0,
pAUC=0.8,graphe=1)
}

for (i in (2:5)) {
  decoup(credit,Duree_credit,Cible,nbmod=i,h=0,k=0,
pAUC=0.8,graphe=1)
}
```

#Regression logistique

```{r}
#formule
predicteurs=grep('(Cle|Cible)',names(credit2))
formule=as.formula(paste("y ~ ",
                         paste(names(credit2),collapse="+")))

#Train and test
set.seed(1234)
train_id=sample(nrow(credit),nrow(credit)*2/3,replace=F)

train=credit2[train_id,]
test=credit2[-train_id,]

train_cible=credit$Cible[train_id]
test_cible=credit$Cible[-train_id]
```

```{r}
#Model
logit=glm(train_cible~1,data=train,family=binomial(link="logit"))

selection=step(logit,direction="forward",trace=TRUE,
               k=log(nrow(train)),scope=list(upper=formule))

selection
```

```{r}
#Calcul AUC du modele obtenu
selection=glm(train_cible~Comptes+Duree_credit+Garanties+Autres_credits+Age,
              data=train,family=binomial(link="logit"))

train.ascbic=predict(selection,newdata=train,type="response")
valid.ascbic=predict(selection,newdata=test,type="response")

library(ROCR)
pred=prediction(train.ascbic,train_cible,label.ordering=c(0,1))
performance(pred,"auc")@y.values[[1]]

pred=prediction(valid.ascbic,test_cible,label.ordering=c(0,1))
performance(pred,"auc")@y.values[[1]]
```

```{r}
#Selection via AIC
selection=step(logit,direction="forward",trace=TRUE,
               k=2,scope=list(upper=formule))

#Calcul AUC du modele obtenu
selection=glm(train_cible~Comptes+Duree_credit+Objet_credit+
                Historique_credit+Epargne+Garanties+Age+
                Autres_credits+Taux_effort+Montant_credit+
                Statut_domicile+Anciennete_emploi,
              data=train,family=binomial(link="logit"))

train.ascbic=predict(selection,newdata=train,type="response")
valid.ascbic=predict(selection,newdata=test,type="response")

library(ROCR)
pred=prediction(train.ascbic,train_cible,label.ordering=c(0,1))
performance(pred,"auc")@y.values[[1]]

pred=prediction(valid.ascbic,test_cible,label.ordering=c(0,1))
performance(pred,"auc")@y.values[[1]]

summary(selection)
```

```{r}
selection=step(logit,direction="forward",trace=TRUE,k=2,
               scope=list(upper=~Comptes+Duree_credit+Historique_credit+
                            Epargne+Garanties+Age+Autres_credits+
                            Taux_effort+Montant_credit+Statut_domicile+
                            Anciennete_emploi))
```

```{r}
#Selection descendante
logit=glm(train_cible~.,data=train,family = binomial)
slection=step(logit,direction="backward",trace=TRUE,k=log(nrow(train)))
```

```{r}
#Leaps and bounds
x=data.frame(model.matrix(~.,data=train))

library(leaps)
y=as.numeric(train_cible)

y[y==1]=0
y[y==2]=1
selec=leaps(x,y,method="Cp",nbest=1,strictly.compatible = F)
plot(selec$size-1,selec$Cp,xlab="#predicteurs",ylab="Cp")

# selec=leaps(x,y,method="adjr2",nbest=1,strictly.compatible=F)
# which(selec$adjr2==max(selec$adjr2))

#Selection du meilleur modele
best.model=selec$which[((selec$Cp==min(selec$Cp))),]
formule=as.formula(paste("y ~",
                         paste(colnames(x)[best.model],collapse="+")))
z=cbind(x,y)
logit=glm(formule,data=z,binomial(link="logit"))
summary(logit)

#Application a l'echantillon test
xt=data.frame(model.matrix(~.,data=test))
prob=predict(logit,newdata=xt,type="response")

pred=prediction(prob,test_cible,label.ordering=c(0,1))
performance(pred,"auc")@y.values[[1]]

selec=leaps(x,y,method="Cp",nbest=10,strictly.compatible=F)
nmodel=nrow(selec$which)
aucfw=matrix(NA,nmodel,3)
models=matrix(NA,nmodel,ncol(selec$which)+1)

for (i in 1:nmodel){
  best.model=selec$which[i,]
  formule=as.formula(paste("y ~",
                           paste(colnames(x)[best.model],
                                 collapse="+")))
  logit=glm(formule,data=z,family=binomial(link="logit"))
  prob=predict(logit,newdata=xt,type="response")
  pred=prediction(prob,test_cible,label.ordering=c(0,1))
  performance(pred,"auc")@y.values[[1]]
  aucfw[i,1]=selec$size[i]-1
  aucfw[i,2]=performance(pred,"auc")@y.values[[1]]
  models[i,1:ncol(selec$which)]=selec$which[i,1:ncol(selec$which)]
  models[i,ncol(selec$which)+1]=performance(pred,"auc")@y.values[[1]]
  aucfw[i,3]=selec$Cp[i]
}

colnames(aucfw)=c("taille","AUC","Cp")
selglob=aucfw[order(aucfw[,2],decreasing=T),]
selglob

```

```{r}
#Selection globale en utilisant regsubsets
fw=regsubsets(train_cible~.,data=train,nbest=1,nvmax=16,really.big=T)
selec=summary(fw)

plot(fw,scale="bic")

plot(apply(selec$which,1,sum)-1,selec$bic,xlab="# predicteurs",ylab="BIC")

#1000 meilleurs modeles
# fw=regsubsets(train_cible~.,data=train,nbest=1000,nvmax=16,really.big=T)
# selec=summary(fw)
# 
# plot(fw,scale="bic")
# 
# plot(apply(selec$which,1,sum)-1,selec$bic,xlab="# predicteurs",ylab="BIC")
```

```{r}
#Fonction pour tester les 1000 modeles
fw=regsubsets(train_cible~.,data=train,nbest=20,nvmax=16,really.big=T)
selec=summary(fw)
x=data.frame(model.matrix(~.,data=train))
y=as.numeric(train_cible)

y[y==1]=0
y[y==2]=1
z=cbind(x,y)
xt=data.frame(model.matrix(~.,data=test))
nmodel=nrow(selec$which)
aucfw=matrix(NA,nmodel,3)
models=matrix(NA,nmodel,ncol(selec$which)+1)

for (i in 1:nmodel){
  best.model=selec$which[i,]
  formule=as.formula(paste("y ~",
                           paste(colnames(x)[best.model],
                                 collapse="+")))
  logit=glm(formule,data=z,family=binomial(link="logit"))
  prob=predict(logit,newdata=xt,type="response")
  pred=prediction(prob,test_cible,label.ordering=c(0,1))
  performance(pred,"auc")@y.values[[1]]
  aucfw[i,1]=apply(selec$which,1,sum)[i]-1
  aucfw[i,2]=performance(pred,"auc")@y.values[[1]]
  models[i,1:ncol(selec$which)]=selec$which[i,1:ncol(selec$which)]
  models[i,ncol(selec$which)+1]=performance(pred,"auc")@y.values[[1]]
  aucfw[i,3]=selec$bic[i]
}

colnames(aucfw)=c("taille","AUC","BIC")
selglob=aucfw[order(aucfw[,2],na.last=NA,decreasing=T),]

head(selglob)

```

```{r,fig.height=5,fig.width=5}
#Recherche des predicteurs maximisant l'aire sous la courbe ROC
colnames(models)=c(names(x),"AUC")
ensmodels=as.data.frame(models)
library(rpart)
cart1=rpart(AUC~.,data=ensmodels,method="anova",parms=list(split="gini"),
           control=list(maxdepth=3))

plot(cart1)
text(cart1)
```

```{r}
#Utilisation de random forest pour l'importance des variables
library(randomForest)
set.seed(235)
rf=randomForest(AUC~.,data=ensmodels,importance=TRUE,ntree=500,
                mtry=6,replace=T,keep.forest=T,nodesize=5)
varImpPlot(rf,cex=0.5)
```

```{r,eval=FALSE}
library(dplyr)
train=train %>% 
  mutate(Cible=train_cible)

test=test %>% 
  mutate(Cible=test_cible)

#Variable predictives
vars=-grep('Cible',names(train))
varx=names(train)[vars]

CombiRegR=function(apprent,validat,varY,varX,p)
{
  y=apprent[,varY] # variable a expliquer
  cible=validat[,varY] #variable a predire
  size=length(varX) # nombre total de variables
  combi=combn(size,p) # combinaison de variables
  predi=matrix(" ",dim(combi)[2],p)
  
  f=function(i)
  {
    #Selection des predicteurs
    s=combi[,i] #ie combinaison de variables
    predicteurs=varX[s]
    
    #ecriture de la formule aves les predicteurs selectionnes
    if (p>1){
      formule=as.formula(paste("y ~",
                               paste(names(apprent[,predicteurs]),collapse="+")))
    } else{
      formule=as.formula(paste("y ~",predicteurs))
    }
    #ajustement du modele logit
    logit=glm(formule,data=apprent,family=binomial(link="logit"))
    #application du modele logit a l'echantillon de test
    scores=predict(logit,newdata=validat,type="response")
    pred=prediction(scores,cible,label.ordering = c(0,1))
    return(list(auc=performance(pred,"auc")@y.values[[1]],predi=predicteurs))
    } #fin de la fonction a vectoriser
  
  cr=matrix(unlist(Vectorize(f)(seq(1,dim(combi)[2]))),
            ncol=p+1,byrow=T)
  
}

system.time(cr<-CombiRegR(apprent=train,validat = test,varY="Cible",varX=varx,p=7))

#affichage des combinaisons par Gini decroissante
resultat=data.frame(cr)
colnames(resultat)=c("AUC",paste("V",1:(ncol(resultat)-1),sep=""))
resultat=resultat[order(resultat$AUC,decreasing=T),]
head(resultat)
```

##Regroupement definitif des modalites

```{r}
#Regroupement

credit2=credit
credit2$Comptes=recode(credit2$Comptes,"'A14'='Pas de compte';
                       'A11'='CC < 0 euros' ; 'A12'='CC [0-200 euros[';
                       'A13'='CC > 200 euros'")
credit2$Historique_credit=recode(credit2$Historique_credit,
                                 "c('A30','A31')='Credit en impaye';
                                 c('A32','A33')='Pas de credits ou en cours sans retard';
                                 'A34'='Credits passes sans retard'")
credit2$Objet_credit=recode(credit2$Objet_credit,
                            "'A40'='Voiture neuve';'A41'='Voiture occasion';
                            c('A42','A43','A44','A45')='Interieur';
                            c('A46','A48','A49','A410')='Etudes-business-Autres';
                            'A47'='Vacances'")
credit2$Epargne=recode(credit2$Epargne,
                       "c('A63','A64','A65')='Pas epargne ou > 500 euros';
                       c('A61','A62')='< 500 euros'")
credit2$Anciennete_emploi=recode(credit2$Anciennete_emploi,
                                 "c('A71','A72')='Sans emploi ou < 1 an';
                                 'A73'='E [1-4[ ans';
                                 c('A74','A75')='E GE 4 ans'")
credit2$Situation_familiale=recode(credit2$Situation_familiale,
                                   "'A91'='Homme divorce/separe';
                                   'A92'='Femme divorcee/separee/mariee';
                                   c('A93','A94')='Homme celibataire/marie/veuf';
                                   'A95'='Femme celibataire'")
credit2$Garanties=recode(credit2$Garanties,"'A103'='Avec garant';
                         else='Sans garant'")
credit2$Biens=recode(credit2$Biens,"'A121'='Immobilier';
                     'A124'='Aucun bien';else='Non immobilier'")
credit2$Autres_credits=recode(credit2$Autres_credits,
                              "'A143'='Aucun credit exterieur';
                              else='Credit exterieurs'")
credit2$Statut_domicile=recode(credit2$Statut_domicile,
                               "'A152'='Proprietaire';
                               else='Non proprietaire'")

credit2$Duree_credit=cut(credit$Duree_credit,c(0,15,36,Inf))

credit2$Age=relevel(cut(credit$Age,c(0,25,Inf)),ref="(25,Inf]")

summary(credit2)

train=credit2[id,]
valid=credit2[-id,]
```

##Modele logit retenu
```{r}
#Modele final
logit=glm(Cible~Comptes+Historique_credit+Duree_credit+Age+Epargne+Garanties+
            Autres_credits,data=train,family=binomial(link="logit"))

summary(logit)
```

```{r}
valid$logit=predict(logit,newdata=valid,type="response")

library(ROCR)
pred=prediction(valid$logit,valid$Cible,label.ordering = c(0,1))
performance(pred,"auc")@y.values[[1]]

#Kolmogorov-Smirnov

plot.ecdf((valid$logit[valid$Cible==0]),main="Fonction de repartition du score",
          col="blue",pch=16)
plot.ecdf((valid$logit[valid$Cible==1]),
          col="red",pch=17,add=T)
legend("bottomright",c("Score=0","Score=1"),pch=c(16,17),
       col=c("blue","red"),lwd=1)

perf=performance(pred,"tpr","fpr")
max(perf@y.values[[1]]-perf@x.values[[1]])

#Maximum sensibilite + specificite
ks=perf@y.values[[1]]-perf@x.values[[1]]
(seuil=pred@cutoffs[[1]][which.max(ks)])
segments(seuil,1-perf@y.values[[1]][which.max(ks)],seuil,
         1-perf@x.values[[1]][which.max(ks)],col="black",lty=3)
```

```{r}
#Fonction de density du score

plot(density(valid$logit[valid$Cible==0]),main="Fonction de densite du score",
     col="blue",xlim=c(-0.2,1.1),ylim=c(0,3),lwd=2)
lines(density(valid$logit[valid$Cible==1]),col="red",lty=3,lwd=2)
legend("topright",c("Score=0","Score=1"),
       lty=c(1,3),col=c("blue","red"),lwd=2)
abline(v=seuil,col="grey")

plot(logit~Cible,data=valid)
```

##Grille de score

```{r}
VARIABLE=c("",gsub("[0-9]","",names(unlist(logit$xlevels))))
MODALITE=c("",as.character(unlist(logit$xlevels)))
names=data.frame(VARIABLE,MODALITE,NOMVAR=c("(Intercept)",paste(VARIABLE,MODALITE,
                                                                sep="")[-1]))

#Recuperation des coefficients du modele
regression=data.frame(NOMVAR=names(coefficients(logit)),
                      COEF=as.numeric(coefficients(logit)))

#Merge des deux data frames
param=merge(names,regression,all.x=TRUE)[-1]
param$COEF[is.na(param$COEF)]=0

#Min max et poids total
mini=aggregate(data.frame(min=param$COEF),by=list(VARIABLE=param$VARIABLE),min)
maxi=aggregate(data.frame(max=param$COEF),by=list(VARIABLE=param$VARIABLE),max)
total=merge(mini,maxi)
total$diff=total$max-total$min
poids_total=sum(total$diff)

grille=merge(param,mini,all.x=TRUE)
grille$delta=grille$COEF-grille$min
grille$POIDS=round(100*grille$delta/poids_total)
grille[which(VARIABLE!=""),c("VARIABLE","MODALITE","POIDS")]
grille[order(grille$VARIABLE,grille$MODALITE)[which(VARIABLE!="")],
       c("VARIABLE","MODALITE","POIDS")]
```

##Determination des seuils de score
```{r}
credit2$logit=predict(logit,newdata=credit2,type="response")
q=quantile(credit2$logit,seq(0,1,by=0.05))
qscore=cut(credit2$logit,q)
tab=table(qscore,credit2$Cible)
ti=prop.table(tab,1)[,2]
barplot(as.numeric(ti),col=gray(0:length(ti)/length(ti)),
        names.arg=names(ti),ylab="Taux impayes",ylim=c(0,1),
        cex.names=0.8,las=3)
abline(v=c(7.3,18.1),col="red")

#Verification
library(car)
zscore=recode(credit2$logit,"lo:0.117='Faible';
              0.117:0.486='Moyen';0.486:hi='Fort'")
table(zscore)
tab=table(zscore,credit2$Cible)
prop.table(tab,1)

# rm(logit)
# decoup(credit2,logit,Cible,nbmod=3,h=1,k=0,pAUC=0.8)
```

##Courbe ROC et courbe de lift

```{r}
#Courbe de ROC
valid$logit=predict(logit,newdata=valid,type="response")
pred=prediction(valid$logit,valid$Cible,label.ordering=c(0,1))
roc=performance(pred,"tpr","fpr")
plot(roc,main="Courbe de ROC")
segments(0,0,1,1,lty=3)
```

```{r}
#Courbe de lift
lift=performance(pred,"tpr","rpp")
plot(roc,main="Courbe de lift")
segments(0,0,prop.table(table(credit$Cible))[2],1,lty=3)
segments(prop.table(table(credit$Cible))[2],1,1,1,lty=3)

#Autre courbe de lift
lift=performance(pred,"lift","rpp")
plot(lift,main="Courbe de lift")
```

##Modele probit

```{r}
probit=glm(Cible~Comptes+Historique_credit+Duree_credit+
             Age+Epargne+Garanties+Autres_credits,
           data=train,family=binomial(probit))

summary(probit)

valid$probit=predict(probit,newdata=valid,type="response")
pred=prediction(valid$probit,valid$Cible,label.ordering = c(0,1))
performance(pred,"auc")@y.values[[1]]

logit$coefficients/probit$coefficients

```

#Regression logistique penalisee ridge

```{r}
library(glmnet)
x=model.matrix(~.-1,data=train[,! names(train) %in% c("Cible","Cle")])
y=train[,"Cible"]

set.seed(235)
cvfit=cv.glmnet(x,y,alpha=0,family="binomial",type="auc",nlambda=100)
cvfit$lambda[1] #plus petit lambda annulant tous les coefficients
cvfit$lambda[99] #lambda precedent divise par 10000
cvfit$lambda.min #lambda donnant le plus petit taux d'erruer
```

##Representation graphique

```{r}
plot(cvfit)
abline(h=cvfit$cvm[which(cvfit$lambda==cvfit$lambda.min)])
abline(v=log(cvfit$lambda.min),col="blue",lty=2)
```

```{r}
fit=glmnet(x,y,alpha=0,family="binomial",
           lambda=seq(cvfit$lambda[1],cvfit$lambda[99],length=10000),
           standardize=TRUE)

plot(fit,xvar="lambda",label="T")
```

##Validation

```{r}
xt=model.matrix(~.-1,data=valid[,! names(valid) %in% c("Cible","Cle","logit","probit")])
yt=valid[,"Cible"]
ytpred=predict(fit,newx=xt,type="response")

#ROC
library(ROCR)
roc=function(x){
  performance(prediction(ytpred[,x],yt),"auc")@y.values[[1]]
}
ptm=proc.time()
vauc=Vectorize(roc)(1:length(fit$lambda))
proc.time()-ptm

#Pararellisation
library(snow)
clus=makeCluster(4)
ptm=proc.time()
clusterExport(clus,c('performance','prediction','ytpred','yt'))
vauc=parSapply(clus,1:length(fit$lambda),'roc')
proc.time()-ptm
stopCluster(clus)

#Identification du meilleur modele
which.max(vauc)

fit$lambda[which.max(vauc)]
vauc[which.max(vauc)]

plot(vauc~log(fit$lambda),lty=2,cex=0.5,pch=16)
abline(v=log(fit$lambda[which.max(vauc)]),col="black",lty=2)
abline(h=0.7806,col="black",lty=3)

tail(log(fit$lambda)[which(vauc>=0.7806)])
tail(fit$lambda[which(vauc>=0.7806)])
```

##Affinement de la recherche du meilleur modele

```{r}
ridge=glmnet(x,y,alpha=0,family="binomial",
             lambda=seq(2,1,-0.0001),standardize=T)
ytpred=predict(ridge,newx=xt,type="response")
vauc=Vectorize(roc)(1:length(ridge$lambda))

vauc[which.max(vauc)]
ridge$lambda[which.max(vauc)]
log(ridge$lambda[which.max(vauc)])

plot(vauc~log(ridge$lambda),lty=2,cex=0.5,pch=16)
abline(h=vauc[which.max(vauc)],col='black',lty=2)
```

##Modele final
```{r}
xt=model.matrix(~.-1,data=credit2[,!names(credit2) %in% c("Cible","Cle","logit","probit")])
yt=credit2[,"Cible"]
ytpred=predict(ridge,newx=xt,type="response",
               s=ridge$lambda[which.max(vauc)])
performance(prediction(ytpred,yt),"auc")@y.values[[1]]

ytpred=predict(ridge,newx=xt,type="response",
               s=1.5)
performance(prediction(ytpred,yt),"auc")@y.values[[1]]

options(scipen=99)
coef(ridge,s=1.5)

```

```{r}
plot(fit,xvar="lambda",label="T")
abline(v=log(1.5),col='black',lty=2)
```

##Grille de score

```{r}
niveaux=Vectorize(levels)(train[,!names(train) %in% c("Cible","Cle")])
niveaux$Taux_effort=""
niveaux$Anciennete_domicile=""
niveaux$Nb_credits=""

VARIABLE=c("",gsub("[0-9]","",names(unlist(niveaux))))
MODALITE=c("",as.character(unlist(niveaux)))
names=data.frame(VARIABLE,MODALITE,NOMVAR=c("(Intercept)",
                                            paste(VARIABLE,MODALITE,sep="")[-1]))
coef=ridge$beta[,which(ridge$lambda==1.5)]
regression=data.frame(NOMVAR=names(coef),
                      COEF=as.numeric(coef))
param=merge(names,regression,al.x=TRUE)[-1]
param$COEF[is.na(param$COEF)]=0
#Calcul du poids total pour normalisation
mini=aggregate(data.frame(min=param$COEF),by=list(VARIABLE=param$VARIABLE),min)
maxi=aggregate(data.frame(max=param$COEF),by=list(VARIABLE=param$VARIABLE),max)
total=merge(mini,maxi)

total$min[total$min==total$max]=0
total$diff=abs(total$max-total$min)
poids_total=sum(total$diff)
grille=merge(param,mini,all.x=TRUE)
grille$delta=grille$COEF-grille$min

grille$delta[grille$VARIABLE=="Anciennete_domicile"]=abs(max(train$Anciennete_domicile)*    grille$COEF[grille$VARIABLE=="Anciennete_domicile"])

grille$delta[grille$VARIABLE=="Nb_credits"]=abs(max(train$Nb_credits)*    grille$COEF[grille$VARIABLE=="Nb_credits"])

grille$delta[grille$VARIABLE=="Taux_effort"]=abs(max(train$Taux_effort)*    grille$COEF[grille$VARIABLE=="Taux_effort"])
grille$POIDS=round((100*grille$delta)/poids_total)
grille[which(grille$VARIABLE!="" & grille$VARIABLE!="Cible"),
       c("VARIABLE","MODALITE","POIDS")]


```

##Affinement du modele ridge

```{r}
#Enlever les variables avec un poids faible

credit2$Nb_pers_charge=NULL
credit2$Anciennete_domicile=NULL
credit2$Telephone=NULL
credit2$Type_emploi=NULL
credit2$Nb_credits=NULL
train=credit2[id,]
valid=credit2[-id,]
x=model.matrix(~.-1,data=train[,!(names(train) %in% c("Cible","Cle","logit"))])
y=train[,"Cible"]
set.seed(235)
cvfit=cv.glmnet(x,y,alpha=0,family="binomial",type="auc",nlambda=100)
fit=glmnet(x,y,alpha=0,family = "binomial",lambda=seq(cvfit$lambda[1],
                                                      cvfit$lambda[99],
                                                      length=10000),
           standardize = T)

#Prediction sur une plage de lambda sur la base de test
xt=model.matrix(~.-1,data=valid[,!(names(train) %in% c("Cible","Cle","logit"))])
yt=valid[,"Cible"]
ytpred=predict(fit,newx=xt,type="response")
roc=function(x){
  performance(prediction(ytpred[,x],yt),"auc")@y.values[[1]]
}
vauc=Vectorize(roc)(1:length(fit$lambda))
vauc[which.max(vauc)]
#Penalisation donnant la plus forte AUC
fit$lambda[which.max(vauc)]

log(vauc[which.max(vauc)])
```

```{r}
#Representation graphique AUC en fonction du log lambda
plot(vauc~log(fit$lambda),lty=2,cex=0.5,pch=16)
abline(v=log(fit$lambda[which.max(vauc)]),col='black',lty=2)
abline(h=vauc[which.max(vauc)],col='black',lty=3)
```

```{r}
pred=prediction(ytpred[,which.max(vauc)],yt,label.ordering=c(0,1))
perf=performance(pred,"tpr","fpr")
max(attr(perf,'y.values')[[1]]-attr(perf,'x.values')[[1]])
```

#Regression logistique penalisee lasso

```{r}
set.seed(235)
cvfit=cv.glmnet(x,y,alpha=1,family="binomial",type="auc",nlambda=100)
cvfit$lambda[1]
length(cvfit$lambda)

#Aire ROC
cvfit$cvm[which(cvfit$lambda==cvfit$lambda.min)]

#Test sur l'echantillon de test
fit=glmnet(x,y,alpha=1,family="binomial",
           lambda=seq(cvfit$lambda[1],cvfit$lambda[length(cvfit$lambda)],
                      length=10000),standardize=T)
plot(fit,xvar="lambda",label=T)
coef(fit,s=log(cvfit$lambda[length(cvfit$lambda)]))

ytpred=predict(fit,newx=xt,type="response")
roc=function(x){
  performance(prediction(ytpred[,x],yt),"auc")@y.values[[1]]
}
vauc=Vectorize(roc)(1:length(fit$lambda))
vauc[which.max(vauc)]

fit$lambda[which.max(vauc)]
```

```{r}
#Resultat graphique de l'analyse AUC
plot(vauc~log(fit$lambda),lty=2,cex=0.5,pch=16)
abline(v=log(fit$lambda[which.max(vauc)]),col='black',lty=2)
abline(h=vauc[which.max(vauc)],col='black',lty=3)

plot(fit,xvar="lambda",label=T)
abline(v=log(fit$lambda[which.max(vauc)]),col='black',lty=2)

pred=prediction(ytpred[,which.max(vauc)],yt,label.ordering=c(0,1))
perf=performance(pred,"tpr","fpr")
max(attr(perf,'y.values')[[1]]-attr(perf,'x.values')[[1]])

#Coefficients du modele correspondant
coef(fit,s=fit$lambda[which.max(vauc)])
```

```{r}
#Recherche de solutions plus parcimonieuses
coef=as.matrix(coef(fit))
coefnul=function(x){sum(coef[,x]!=0)-1}
coefn0=Vectorize(coefnul)(1:length(fit$lambda))
lasso=cbind(coefn0,vauc)
head(lasso)
tail(lasso)

#AUC maximale pour chaque nombre de variables
aggregate(lasso[,"vauc"],by=list(lasso[,"coefn0"]),max)
```

##Non-consistance de l'estimateur lasso

```{r}
x=model.matrix(~.,data=train[,!(names(train) %in% c("Cle","logit","Cible"))])
y=train[,"Cible"]
xt=model.matrix(~.,data=valid[,!(names(train) %in% c("Cle","logit","Cible"))])
yt=valid[,"Cible"]

#OLS et calcul de l'inverse des coefficients
beta.ols=lm(y~x,data=train)$coefficients[-1]
penal=pmax(1,abs(beta.ols^(-1)),na.rm=TRUE)

fit=glmnet(x,y,alpha=1,family="binomial",
           lambda=seq(cvfit$lambda[1],cvfit$lambda[length(cvfit$lambda)],
                      length=10000),standardize = T,penalty.factor = penal)
ytpred=predict(fit,newx=xt,type="response")
toc=function(x){
  performance(prediction(ytpred[,x],yt),"auc")@y.values[[1]]
}
vauc=Vectorize(roc)(1:length(fit$lambda))
vauc[which.max(vauc)]

coef(fit,s=fit$lambda[which.max(vauc)])
```

##Group-lasso

```{r}
library(grplasso)
library(magrittr)
0.002853051*length(id)

#Coefficient minimum annukant tous les coefficients
aggregate(fit$lambda,by=list(lasso[,"coefn0"]),min)[1,2]*length(id)
lambda=seq(100,1,length=1000)
credit3=credit2 %>% 
  dplyr::select(-Cle,-logit)
credit3$Cible=as.numeric(credit2$Cible)-1
gplasso=grplasso(Cible~.,data=credit3,lambda=lambda,model=LogReg())

coe=coefficients(gplasso)
modannul=function(i){sort(rownames(coe)[(coe[,i]!=0)])}
for (j in (length(lambda)-1):1)
{
  if (length(modannul(j))<length(modannul(j+1)))
  {
    cat("\n",j,modannul(j+1)[-match(modannul(j),modannul(j+1))])
  }
}
```

```{r}
#Representation graphique de l'evolution des coefficients
plot(gplasso)
abline(v=lambda[71],lty=3)
lambda[71]
```

#Penalisation elastic net

```{r}
elastic=function(a){
  set.seed(235)
  cvfit=cv.glmnet(x,y,alpha=a,family="binomial",type="auc",
                  nlambda=100)
  #Calcul de la regression pour une plage de valeurs de lambda
  fit=glmnet(x,y,alpha=a,family="binomial",lambda=seq(cvfit$lambda[1],
                                                      cvfit$lambda[length(cvfit$lambda)],
                                                      length=10000),standardize = T)
  #prediction sur une plagede lambda sur la base de test
  ytpred=predict(fit,newx=xt,type="response")
  roc=function(x){
    performance(prediction(ytpred[,x],yt),"auc")@y.values[[1]]
  }
  vauc=Vectorize(roc)(1:length(fit$lambda))
  return(vauc[which.max(vauc)])
}
time=proc.time()
for (i in seq(0,1,by=0.1)){
  cat("\n value=",i," --- AUC:",elastic(i),"\n")
}
proc.time()-time

```

#Regression PLS

```{r}
library(plsRglm)
x=model.matrix(~.-1,data=train[,!(names(train) %in% c("Cle","logit","Cible"))])
y=as.numeric(train[,"Cible"])
#recodage en 0/1
y[y==1]=0
y[y==2]=1

#Echantillon test
xt=model.matrix(~.-1,data=valid[,!(names(valid) %in% c("Cle","logit","Cible"))])
yt=as.numeric(valid[,"Cible"])

#recodage en 0/1
yt[yt==1]=0
yt[yt==2]=1

```

```{r}
#Regression
pls1=plsRglm(y,x,nt=1,dataPredictY=xt,modele="pls-glm-logistic")

pred=prediction(pls1$ValsPredictY,valid$Cible,label.ordering=c(0,1))
performance(pred,"auc")@y.values[[1]]

pls1$InfCrit

pls1$Std.Coeffs

pls1$Coeffs
```

```{r}
#Test des modeles avec plus de composantes

pls=plsRglm(y,x,nt=2,dataPredictY=xt,modele="pls-glm-logistic")
pred=prediction(pls$ValsPredictY,valid$Cible,label.ordering=c(0,1))
performance(pred,"auc")@y.values[[1]]

pls=plsRglm(y,x,nt=3,dataPredictY=xt,modele="pls-glm-logistic")
pred=prediction(pls$ValsPredictY,valid$Cible,label.ordering=c(0,1))
performance(pred,"auc")@y.values[[1]]

pls=plsRglm(y,x,nt=4,dataPredictY=xt,modele="pls-glm-logistic")
pred=prediction(pls$ValsPredictY,valid$Cible,label.ordering=c(0,1))
performance(pred,"auc")@y.values[[1]]

pls=plsRglm(y,x,nt=10,dataPredictY=xt,modele="pls-glm-logistic")
pls$InfCrit
```

##Incoherence de signes

```{r}
pls2=plsRglm(y,x,nt=2,dataPredictY = xt,modele="pls-glm-logistic")
rownames(pls1$Coeffs)[which(pls1$Coeffs*pls2$Coeffs<0)]
pls1$Coeffs[which(pls1$Coeffs*pls2$Coeffs<0)]
pls2$Coeffs[which(pls1$Coeffs*pls2$Coeffs<0)]

prop.table(table(train$Objet_credit,train$Cible),1)
cor(x,y)*pls1$Coeffs[-1]
```

##Estimateurs bootstrap
```{r}
pls1$family
bootpls1=bootpls(pls1,R=1000)
```

```{r}
#Representation graphique
boxplots.bootpls(bootpls1,prednames=FALSE)

plots.confints.bootpls(confints.bootpls(bootpls1,typeBCa=F))

```

#Arbre de decision CART

```{r}
#Example
library(tree)
arbre=tree(Cible~Age+Duree_credit,data=credit)
partition.tree(arbre)
arbre
```

```{r}
library(rpart)
set.seed(235)
cart=rpart(Cible~.,data=credit[id,vars],
           method="class",parms=list(split="gini"),cp=0)
cart=rpart(Cible~.,data=credit[id,c(varquali,varquanti,"Cible")],method="class",
           parms=list(split="gini"),control=list(minbucket=30,minsplit=30*2,maxdepth=4))
cart
summary(cart)
```

```{r}
#Graph
plot(cart,branch=.2,uniform=T,compress=T,margin=.1)
text(cart,fancy=T,use.n=T,pretty=0,all=T,cex=.6)
```

##Elagage

```{r}
vars=names(credit)[!(names(credit) %in% c("Cle","logit"))]
cart=rpart(Cible~.,data=credit[id,vars],
           method="class",parms=list(split="gini"),cp=0)
printcp(cart)

sum(predict(cart,type="class")!=credit[id,"Cible"])/nrow(credit[id,])
plotcp(cart)

summary(cart)
```

```{r}
#Pruned cart tree
prunedcart4f=prune(cart,cp=0.0312500)
plot(prunedcart4f,branch=.2,uniform=T,compress=T,margin=.1)
text(prunedcart4f,fancy=T,use.n=T,pretty=0,all=T,cex=.5)

#Pruned tree with rattle package
library(rattle)
fancyRpartPlot(prunedcart4f,sub=" ",palettes=c("Greys"))
```

```{r}
#Elagage de l'arbre
library(rpart)
xerr=cart$cptable[,"xerror"]
minxerr=which.min(xerr)
seuilerr=cart$cptable[minxerr,"xerror"]+cart$cptable[minxerr,"xstd"]
xerr[xerr<seuilerr][1]
mincp=cart$cptable[names(xerr[xerr<seuilerr][1]),"CP"]
mincp
prunedcart=prune(cart,cp=mincp)
```

```{r}
#stump
cart=rpart(Cible~.,data=credit[id,vars],
           method="class",parms=list(split="gini"),control=list(maxdepth=1,
                                                                cp=-1,minsplit=0))
plot(cart)
```

##Prediction et mesure du pouvoir discriminant d'un arbre de decision

```{r}
test=credit[-id,vars]
test$CART4f=predict(prunedcart4f,newdata=test,type="prob")

library(ROCR)
pred=prediction(test$CART4f[,2],test$Cible,label.ordering=c(0,1))
auc=performance(pred,"auc")
auc
```

```{r}
set.seed(235)
auc=matrix(NA,nrow(cart$cptable)-1,4)
for (i in 2:nrow(cart$cptable)){
  cartp=prune(cart,cp=cart$cptable[i,"CP"])
  predc=predict(cartp,type="prob",test)[,2]
  pred=prediction(predc,test$Cible,label.ordering = c(0,1))
  auc[i-1,1]=cart$cptable[i,"CP"]
  auc[i-1,2]=cart$cptable[i,"nsplit"]+1
  auc[i-1,3]=cart$cptable[i,"xerror"]
  auc[i-1,4]=performance(pred,"auc")@y.values[[1]]
}#fin de boucle
colnames(auc)=c("CP","nfeuilles","erreur","AUC")
```

```{r}
#Courbe ROC avec CI pour les "meilleurs" modeles
prunedcart5f=prune(cart,cp=0.026041667)
test$CART5f=predict(prunedcart5f,newdata=test,type="prob")

prunedcart7f=prune(cart,cp=0.018229167)
test$CART7f=predict(prunedcart7f,newdata=test,type="prob")

library(pROC)
roc=plot.roc(test$Cible,test$CART5f[,2],col="black",lty=1,ci=TRUE)
plot.roc(test$Cible,test$CART7f[,2],col="red",add=TRUE,lty=2,ci=TRUE)
roc.se=ci.se(roc,specificities = seq(0,1,.01))
plot(roc.se,type="shape",col="#0000ff22")
legend("bottomright",c('5 feuilles','7feuilles'),
       col=c('black','red'),lty=c(1,2),lwd=3)
```

#Algorithme PRIM

##En deux dimensions
```{r}
x=credit[,c("Duree_credit","Age")]
y=as.numeric(credit[,"Cible"])
y[y==1]=0
y[y==2]=1

library(prim)
prim=prim.box(x,y,peel.alpha=0.5,paste.alpha=0.01,mass.min=0.05,
              pasting=T,verbose=T,threshold.type=1,threshold=0.3)

summary(prim,print.box=T)

plot(prim,col="transparent")
points(x[y==0,],pch=1)
points(x[y==1,],pch=17)
```

##PRIM sur les variables qualitatives

```{r}
x=credit[,varquanti]
prim=prim.box(x,y,peel.alpha = 0.05,paste.alpha = 0.01,mass.min = 0.05,
              pasting=T,verbose=T,threshold.type = 1,threshold = 0.3)
```

```{r}
summary(prim,print.box=T)
```

```{r}
#Taux alpha plus important
prim=prim.box(x,y,peel.alpha = 0.5,paste.alpha = 0.01,mass.min = 0.05,
              pasting=T,verbose=T,threshold.type = 1,threshold = 0.3)
summary(prim,boxplot=T)
```

##PRIM sur les variables quantitatives et qualitatives

```{r}
varquanti2=names(credit)[names(credit) %in% varquanti]
varquali2=names(credit2) %in% varquali
formule=as.formula(paste("y= ~ 0 +",#0 est mis pour ne pas tenir compte de la constante
                         paste(names(credit2[,varquali2]),collapse="+")))
x=cbind(credit[,varquanti2],data.frame(model.matrix(formule,data=credit2)))

prim=prim.box(x,y,peel.alpha = 0.5,paste.alpha = 0.01,mass.min = 0.05,
              pasting=T,verbose=T,threshold.type = 1,threshold = 0.3)
summary(prim,print.box=T)
```

```{r}
#Aire ROC
x=cbind(credit[id,varquanti2],
        data.frame(model.matrix(formule,data=credit2[id,])))
y=as.numeric(credit[id,"Cible"])
xt=cbind(credit[-id,varquanti2],
        data.frame(model.matrix(formule,data=credit2[-id,])))
yt=as.numeric(credit[-id,"Cible"])
library(pROC)
auc(y,prim.which.box(x,prim))
auc(yt,prim.which.box(xt,prim))

#Fonction testant differentes valeurs d'alpha
aucprim=matrix(NA,10,4)
j=1
for (n in seq(0.05,0.5,by=0.05)){
  prim=prim.box(x,y,peel.alpha=n,paste.alpha=0.01,mass.min=0.05,
                pasting=T,verbose=F,threshold.type=1,threshold=0.3)
  aucprim[j,1]=n
  aucprim[j,2]=prim$num.class
  aucprim[j,3]=auc(y,prim.which.box(x,prim))
  aucprim[j,4]=auc(yt,prim.which.box(xt,prim))
  j=j+1
}
colnames(aucprim)=c("alpha","nb boites","AUC train","AUC test")
aucprim
```

##PRIM sur les coordonnees factorielles des variables explicatives

```{r}
library(FactoMineR)
#Formattage des donnees
credit2$Taux_effort=NULL
credit2$Anciennete_emploi=NULL
credit2$Nb_credits=NULL
Montant_credit=cut(credit$Montant_credit,c(0,4000,Inf))
credit2$Montant_credit=Montant_credit

ACM=MCA(credit2[,1:13],ncp=32,axes=c(1,2),graph=TRUE,quali.sup=13)
ACM$eig

```

```{r}
barplot(ACM$eig[,2],names=paste("Dim",1:nrow(ACM$eig)))

plot(ACM,choix="ind",invisible="ind",xlim=c(-1,2),
     autoLab="yes",cex=0.7)

plotellipses(ACM,means=T,level=0.95,keepvar="Cible",label='quali')
```
}
#Creation des donnees d'apprentissage
##Training data
x=ACM$ind$coord[id,1:32]
y=as.numeric(credit[id,"Cible"])
y[y==1]=0
y[y==2]=1

##Test data
xt=ACM$ind$coord[-id,1:32]
yt=as.numeric(credit[-id,"Cible"])
yt[yt==1]=0
yt[yt==2]=1

##Fonction d'estimation du modele PRIM et calcul de AUC
library(prim)
library(pROC)
f=function(i,j){
  prim=prim.box(x,y,peel.alpha = i,paste.alpha = j,
                mass.min=0.05,pasting=T,verbose=F,threshold.type = 1,
                threshold=0.3)
  auc(yt,prim.which.box(xt,prim))
}

for (n in seq(2,12,by=1)){
  i=seq(0.05,0.5,by=0.05)
  j=seq(0.01,0.1,by=0.01)
  x=ACM$ind$coord[id,1:n]
  xt=ACM$ind$coord[-id,1:n]
  k=outer(i,j,Vectorize(f))
  alpha1=i[which(k==max(k),arr.ind = TRUE)[1,1]]
  alpha2=j[which(k==max(k),arr.ind = TRUE)[1,2]]
cat("\n","nb facteurs=",n ," AUC test max=",max(k)," alpha peel =",alpha1,"alpha paste = ",
    alpha2)
}
```

```{r}

```{r
```

#Forets aleatoires

```{r}
#Exemple basique
library(randomForest)
set.seed(235)
var2=c(varquali,varquanti)
var3=c(varquali,varquanti,"Cible")
rf=randomForest(Cible~.,data=credit[id,var3],importance=TRUE,ntree=500,
                mtry=3,replace=T,keep.forest=T,
                nodesize=5,ytest=credit[-id,"Cible"],xtest=credit[-id,var2])
rf
```

```{r}
#Changement du cut-off
rf=randomForest(Cible~.,data=credit[id,var3],importance=TRUE,ntree=500,
                mtry=3,replace=T,keep.forest=T,
                nodesize=5,ytest=credit[-id,"Cible"],xtest=credit[-id,var2],
                cutoff=c(0.5,0.25),proximity=TRUE)
rf
##Number of times as OOB
head(rf$oob.times)

##Resultat des votes OOB
head(rf$votes)

rf$pred=ifelse(rf$votes[,2]>=0.5,1,0)
table(rf$pred)

err_oob=sum(rf$pred!=credit[id,]$Cible)/nrow(credit[id,])

err_oob
```

```{r}
#Representation graphique de l'erreur
plot(rf$err.rate[,1],type='l',ylim=c(.2,.4),xlab="nombre d'iterations",
     ylab='erreur')
lines(rf$test$err.rate[,1],type='l',lwd=2,col='red')

#Nombre d'iterations pour lequel le minimum d'erreur est atteint
min.err=min(rf$err.rate[,"OOB"])
min.err.idx=which(rf$err.rate[,"OOB"]==min.err)
min.err.idx
```

```{r}
test=predict(rf,credit[-id,var2],type='prob')[,2]
pred=prediction(test,credit[-id,"Cible"],label.ordering = c(0,1))
performance(pred,"auc")@y.values[[1]]

trainrf=predict(rf,credit[id,],type='response')
err_train=sum(trainrf!=credit[id,"Cible"])/nrow(credit[id,])
err_train
```

```{r}
#Erreur .632+
p=table(credit[id,"Cible"])[2]/nrow(credit[id,])
q=table(trainrf)[2]/nrow(credit[id,])
gamma=p*(1-q)+(1-p)*q

txoverfit=(err_oob-err_train)/(gamma-err_train)
txoverfit
w=0.632/(1-0.368*txoverfit)
(1-w)*err_train+w*err_oob
```

##Graphique de proximite
```{r}
library(randomForest)
MDSplot(rf,credit[id,vars]$Cible,pch=as.numeric(credit[id,vars]$Cible))
```

##Mesure de l'importance des variables

```{r}
#Mean Decrease accuracy
importance(rf,type=1)[order(importance(rf,type=1),decreasing=TRUE),]

#Mean Decrease Gini
importance(rf,type=2)

varImpPlot(rf)
```

```{r}
#Autre representation graphique
rf.imp=importance(rf,type=1)[order(importance(rf,type=1),decreasing=TRUE),]
par(mar=c(8,4,4,0))
barplot(rf.imp,col=gray(0:nrow(rf$importance)/nrow(rf$importance)),
        ylab='Importance',ylim=c(0,30),
        cex.names=0.8,las=3)
```

```{r,fig.width=6}
#Calcul de l'importance moyenne
nvar=ncol(credit[,vars])-1
vimp=rep(0,nvar)
nsimul=30
for (i in 1:nsimul){
  rf=randomForest(Cible~.,data=credit[id,vars],
                  importance=TRUE,ntree=500,mtry=3,replace=T,
                  keep.forest=T,nodesize=5)
  vip=importance(rf,type=1)
  vimp=vimp+vip[order(rownames(vip))]/nsimul
  rm(rf)
}
a1=sort(rownames(vip))
a2=order(vimp,decreasing=TRUE)
par(mar=c(8,4,4,0))
barplot(vimp[a2],col=gray(0:nvar/nvar),names.arg=a1[a2],
        ylab='Importance',ylim=c(0,30),cex.names=0.8,las=3)
title(main=list("randomForest : Importance moyenne des variables",
                font=1,cex=1.2))
```

##Choix du nombre de variables

```{r}
mtry=tuneRF(credit[id,-which(names(credit) %in% c('Cible','Cle'))],credit[id,which(names(credit) %in% c('Cible'))],
            mtryStart=1,ntreeTry=500,stepFactor=2,improve=0.001)
```

```{r}
#Choix de maniere manuelle
set.seed(235)
nsimul=100
nvarmin=1
nvarmax=6
auc=matrix(NA,nvarmax-nvarmin+1,2)
for (nvar in nvarmin:nvarmax){
  auc[nvar-nvarmin+1,1]=nvar
  rft=matrix(NA,nrow(credit[-id,var2]),nsimul+1)
  for(i in 1:nsimul){
    rf=randomForest(Cible~.,data=credit[id,var3],
                    importance=F,ntree=500,mtry=nvar,replace=T,
                    keep.forest=T,nodesize=5)
    rft[,i]=predict(rf,credit[-id,var2],type='prob')[,2]
  }
  rft[,nsimul+1]=apply(rft[,1:nsimul],1,mean)
  pred=prediction(rft[,nsimul+1],credit[-id,"Cible"],
                  label.ordering=c(0,1))
  auc[nvar-nvarmin+1,2]=performance(pred,"auc")@y.values[[1]]
}
colnames(auc)=c("nb variables","AUC test")

```

##RandomForest sans tirage aleatoire des observations
```{r}
set.seed(235)
rf=randomForest(Cible~.,data=credit[id,var3],
                    importance=TRUE,ntree=500,mtry=2,replace=F,
                    sampsize=length(id),keep.forest=T,nodesize=5,
                ytest=credit[-id,"Cible"],xtest=credit[-id,var2])
rf_test=predict(rf,credit[-id,var2],type="prob")[,2]
pred=prediction(rf_test,credit[-id,"Cible"],label.ordering = c(0,1))
performance(pred,"auc")@y.values[[1]]
```

##Extra-Trees

```{r}
library(extraTrees)
x=model.matrix(~.-1,data=credit[id,!names(credit) %in% c("Cible","Cle")])
y=credit[id,"Cible"]
xt=model.matrix(~.-1,data=credit[-id,!names(credit) %in% c("Cible","Cle")])
et=extraTrees(x,y,ntree=500,mtry=1,numRandomCuts=1,nodesize=5,numThreads=6)
et
pred.et=predict(et,xt,probability = T)[,2]
pred=prediction(pred.et,credit[-id,"Cible"],label.ordering=c(0,1))
performance(pred,"auc")@y.values[[1]]
```

```{r}
#Benchmarking
library(microbenchmark)
microbenchmark(extraTrees(x,y,ntree=500,mtry=5,numRandomCuts=1,nodesize=5,numThreads = 1),
               times=30)
microbenchmark(extraTrees(x,y,ntree=500,mtry=5,numRandomCuts=1,nodesize=5,numThreads = 6),
               times=30)
```


#Le bagging
```{r}
library(rpart)
library(ipred)
set.seed(235)
bag1=bagging(Cible~.,data=credit[id,!names(credit) %in% c("Cle")],nbagg=200,
             coob=TRUE,control=rpart.control(minbucket=5))
bag1

#Moyenne des probabilites votant 0 ou 1
test_bag1=predict(bag1,credit[-id,!names(credit) %in% c("Cible","Cle")],type="prob",
                              aggregation="average")

head(test_bag1)

pred=prediction(test_bag1[,2],credit[-id,"Cible"],label.ordering = c(0,1))
performance(pred,"auc")@y.values[[1]]

#Proportion des arbres votant 0 ou 1
test_bag1=predict(bag1,credit[-id,!names(credit) %in% c("Cible","Cle")],type="prob",
                              aggregation="majority")
head(test_bag1)
pred=prediction(test_bag1[,2],credit[-id,"Cible"],label.ordering = c(0,1))
performance(pred,"auc")@y.values[[1]]
```

```{r}
#Test sur les arbres stumps (maxdepth=1, cp=-1)
set.seed(235)
stump=bagging(Cible~.,data=credit[id,!names(credit) %in% c("Cle")],nbagg=200,
             coob=TRUE,control=rpart.control(maxdepth=1,cp=-1))
test_stump=predict(stump,credit[-id,!names(credit) %in% c("Cible","Cle")],
                   type="prob",aggregation="average")
pred=prediction(test_stump[,2],credit[-id,"Cible"],label.ordering = c(0,1))
performance(pred,"auc")@y.values[[1]]

test_stump=predict(stump,credit[-id,!names(credit) %in% c("Cible","Cle")],
                   type="prob",aggregation="majority")
pred=prediction(test_stump[,2],credit[-id,"Cible"],label.ordering = c(0,1))
performance(pred,"auc")@y.values[[1]]
```

```{r}
#Mauvais example stump avec bagging

##Arbre de regression CART
cart=rpart(Cible~Anciennete_domicile+Telephone+Nb_pers_charge,data=credit[id,!names(credit) %in% c("Cle")],method="class",parms=list(split="gini"),cp=-1,maxdepth=1)
cart
predc=predict(cart,type="prob",credit[-id,!names(credit) %in% c("Cible","Cle")])[,2]
pred=prediction(predc,credit[-id,"Cible"],label.ordering=c(0,1))
performance(pred,"auc")@y.values[[1]]

##Bagging
set.seed(235)
bag1=bagging(Cible~Anciennete_domicile+Telephone+Nb_pers_charge,
             data=credit[id,!names(credit) %in% c("Cle")],
             nbagg=200,coob=TRUE,control=rpart.control(maxdepth=1,cp=-1))
predc=predict(bag1,credit[-id,!names(credit) %in% c("Cible","Cle")],type="prob",
              aggregation="average")
pred=prediction(predc[,2],credit[-id,"Cible"],label.ordering=c(0,1))
performance(pred,"auc")@y.values[[1]]
```

#Forets aleatoires de modeles logistiques

```{r}
#Fonction pour realiser les forets aleatoires
RForestLog=function(apprent,validat,varY,varX,nb_var,
                    crit="bic",nsimul){
  nobs=nrow(apprent)
  
  #ajustement d'un modele sature pour obtenir l'ensemble des modalites dont nous
  #stockerons les coefficients
  y=apprent[,varY]
  formule=as.formula(paste("y ~",paste(names(apprent[,varX]),collapse="+")))
  sature=glm(formule,data=apprent[,c(varX,varY)],family=binomial(link="logit"))
  coef=matrix(0,length(coef(sature)),nsimul+2)
  rownames(coef)=names(coef(sature))
  
  #matrice contenant les predictions de chaque modele sur l'echantillon test
  predic=matrix(NA,nrow(validat),nsimul)
  #vecteur contenant les AUC de chaque modele
  auc=matrix(0,nsimul,1)
  
  for (i in (1:nsimul)) #boucle sur le nb d'iterations
  {
    #tirage aleatoire simple des variables
    size=length(varX) #nb de variables explicatives
    s=sort(sample(size,nb_var,replace=F))
    predicteurs=varX[s]
    
    #tirage aleatoire equiprobable avec remise des individus dans
    #l'echantilllon d'apprentissage
    s=sample(nobs,nobs,replace=T)
    
    #ecriture d'une formule avec l'ensemble des predicteurs selectionnes aleatoirement
    if (nb_var>1){
      formule=as.formula(paste("y ~",paste(names(apprent[,predicteurs]),collapse="+")))
    } else {
      formule=as.formula(paste("y ~",predicteurs))
    }
    cible=apprent[s,varY]
    #modele minumum
    logit=glm(cible~1,data=apprent[s,],family=binomial(link="logit"))
    
    #Selection pas a pas sur la base du critere BIC ou AIC
    if (crit=="bic"){
      selection=step(logit,direction="forward",trace=F,
                     k=log(nobs),scope=list(upper=formule))
    } else {
      selection=step(logit,direction ="forward",trace=F,
                     k=2,scope=list(upper=formule))
    }
    
    #application du modele logit ajuste a l'echantillon de test
    predic[,i]=predict(selection,newdata=validat,type="response")
    
    #moyenne des predictions des i premiers modeles agreges
    if (i==1){
      moypred=predic[,i]
    } else {
      moypred=apply(predic[,1:i],1,mean)
    }
    #calcul de l'AUC des i premiers modeles agreges
    cible=validat[,varY]
    pred=prediction(moypred,cible,label.ordering=c(0,1))
    auc[i]=performance(pred,"auc")@y.values[[1]]
    
    #stockage des coefficients non nuls
    index=match(names(coef(selection)),names(coef(sature)))
    coef[index,i]=coef(selection)
  } #Fin de la boucle
  
  #coefficients moyens des modeles agreges
  coef[,nsimul+1]=apply(coef[,1:nsimul],1,mean)
  #nb coefficients non nuls par modalite
  coef[,nsimul+2]=apply((coef[,1:nsimul]!=0),1,sum)
  
  #resultats en retour de la fonction
  rf=list(auc,coef)
  
                    } #fin de la fonction
```

```{r}
#Application de la fonction RForestLog
library(car)
credit2$Objet_credit=recode(credit$Objet_credit,"'A40'='Voiture neuve';
                            'A41'='Voiture occasion';
                            c('A42','A44','A45')='Intérieur';
                            'A43'='Vidéo - HIFI';c('A46','A48','A410')='Etudes';
                            'A47'='Vacances';'A49'='Business'; else='Autres'")

niter=300
varx=c(varquali,varquanti)
index=match(names(credit2[id,]),varx)
varx2=varx[na.omit(index)]
rf=RForestLog(apprent=credit2[id,],validat=credit2[-id,],varY="Cible",
              varX=varx2,nb_var=4,crit="aic",nsimul=niter)

#Plot AUC
plot(rf[[1]])
rf[[2]][,niter+1]
rf[[2]][,niter+2]
```

```{r}
#Ensemble
Ensemble=function(apprent,validat,varY,varX,nb_var,crit="bic",nsimul,nb){
  y=apprent[,varY]
  formule=as.formula(paste("y~ ",paste(names(apprent[,varX]),collapse="+")))
  sature=glm(formule,data=apprent[,c(varX,varY)],
             family=binomial(link="logit"))
  coe=matrix(0,length(coef(sature)),nb+1)
  rownames(coe)=names(coef(sature))
  occ=matrix(0,length(coef(sature)),nb+1)
  rownames(occ)=names(coef(sature))
  auc=matrix(0,nsimul,nb+1)
  
  for (i in (1:nb)){
    rf=RForestLog(apprent,validat,varY,varX,nb_var,crit,nsimul)
    auc[,i]=rf[[1]]
    coe[,i]=rf[[2]][,nsimul+1]
    occ[,i]=rf[[2]][,nsimul+2]
  }
  
  #moyenne des AUC
  auc[,nb+1]=apply(auc[,1:nb],1,mean)
  
  #moyenne des coefficients
  coe[,nb+1]=apply(coe[,1:nb],1,mean)
  
  #moyenne des nb d'occurrences
  occ[,nb+1]=apply(occ[,1:nb],1,mean)
  
  #resultats en retour
  ens=list(auc,coe)
} #fin de la fonction
```

```{r}
#Utilisation de la fonction
niter=300
n=30
ens=Ensemble(apprent=credit2[id,],validat=credit2[-id,],varY="Cible",
              varX=varx2,nb_var=3,crit="aic",nsimul=niter,nb=n)

#Plot AUC
plot(ens[[1]][,31])

#Coefficients
ens[[2]][,31]

```

#Le boosting

```{r}
library(ada)
set.seed(235)
boost=ada(Cible~.,data=credit[id,c(var2,"Cible")],type="discrete",
          loss="exponential",control=rpart.control(cp=0),iter=5000,
          nu=1,test.y=credit[-id,"Cible"],test.x=credit[-id,var3])
summary(boost)
```

```{r}
#Tracage du suivi de l'erreur en fonction des iterations
plot(boost,kappa=F,test=T,tflag=F)
```

```{r}
#Modele sur des stumps
boost=ada(Cible~.,data=credit[id,c(var2,"Cible")],type="discrete",
          loss="exponential",control=rpart.control(cp=-1,maxdepth=1,minsplit=0),iter=5000,
          nu=1,test.y=credit[-id,"Cible"],test.x=credit[-id,var3])
summary(boost)

plot(boost,kappa=F,test=T)
```

```{r}
#Ajout d'une penalisation
boost=ada(Cible~.,data=credit[id,c(var2,"Cible")],type="discrete",
          loss="exponential",control=rpart.control(cp=0),iter=5000,
          nu=0.01,test.y=credit[-id,"Cible"],test.x=credit[-id,var3])
boost
summary(boost)
```

##Real AdaBoost

```{r}
#Real AdaBoost
library(ada)
boost=ada(Cible~.,data=credit[id,c(var2,"Cible")],type="real",
          loss="exponential",control=rpart.control(cp=0),iter=5000,
          nu=0.01,test.y=credit[-id,"Cible"],test.x=credit[-id,var3])
boost
summary(boost)
```

```{r}
#Prediction
nsimul=30
auc=matrix(NA,nsimul+1,1)
bst=matrix(NA,nrow(credit[-id,]),nsimul+1)
for (i in 1:nsimul){
  rm(boost)
  boost=ada(Cible~.,data=credit[id,c(var2,"Cible")],type="real",
            loss="exponential",control=rpart.control(cp=0),iter=2000,
            nu=0.01,test.y=credit[-id,"Cible"],test.x=credit[-id,var3])
  bst[,i]=predict(boost,credit[-id,var3],type="prob")[,2]
  pred=prediction(bst[,i],credit[-id,"Cible"],label.ordering=c(0,1))
  auc[i,1]=performance(pred,"auc")@y.values[[1]]
} # Fin de boucle
bst[,nsimul+1]=apply(bst[,1:nsimul],1,mean)
pred=prediction(bst[,nsimul+1],credit[-id,"Cible"],label.ordering = c(0,1))
auc[nsimul+1,1]=performance(pred,"auc")@y.values[[1]]
auc
```

##Boosting de modeles logistiques

```{r}
#Recodage de la variable objet credit
library(car)
credit2$Objet_credit=recode(credit$Objet_credit,"'A40'='Voiture neuve';
                            'A41'='Voiture occasion';
                            c('A42','A44','A45')='Intérieur';
                            'A43'='Vidéo - HIFI';c('A46','A48','A410')='Etudes';
                            'A47'='Vacances';'A49'='Business'; else='Autres'")

BoostLog=function(apprent,validat,varY,varX,crit="bic",shrink=1,nsimul){
  nobs=nrow(apprent)
  w=rep(1/nobs,nobs)
  rep=0
  y=apprent[,varY]
  formule=as.formula(paste("y ~",paste(names(apprent[,varX]),collapse="+")))
  sature=glm(formule,data=apprent[,c(varX,varY)],family=binomial(link="logit"))
  coef=matrix(0,length(coef(sature)),nsimul+2)
  rownames(coef)=names(coef(sature))
  predic=matrix(NA,nrow(validat),nsimul)
  auc=matrix(0,nsimul,1)
  
  for (i in (1:nsimul)){
    #graine pour tirage aleatoire
    set.seed(seed)
    
    #initialisation des poids
    w=w/sum(w,na.rm=TRUE)
    #prevoir le cas ou un poids w est si petit qu'il vaut NaN
    
    #variables explicatives
    nb_var=length(varX)
    predicteurs=varX
    
    #tirage aleatoire pondere avec remise de l'echantillon d'apprentissage
    s=sample(nobs,nobs,replace=T,prob=w)
    
    #creation de la formule avec l'ensemble des predicteurs
    if (nb_var>1){
      formule=as.formula(paste("y~ ",paste(names(apprent[,predicteurs]),collapse="+")))
    } else {
      formule=as.formula("y ~",predicteurs)
    }
    cible=apprent[s,varY]
    
    #selection ascendante pas a pas
    logit=glm(cible~1,data=apprent[s,],family=binomial(link="logit"))
    if (crit=="bic"){
      selection=step(logit,direction="forward",trace=F,k=log(nobs),scope=list(upper=formule))
    } else {
      selection=step(logit,direction="forward",trace=F,k=2,scope=list(upper=formule))
    }
  
    #application du modele logit a l'echantillon d'apprentissage et MAJ des poids
    pred=predict(selection,newdata=apprent,type="response")
    alpha=log(pred/(1-pred))/2
    signe=ifelse(apprent[,varY]==1,1,-1)
    w=w*exp(-shrink*signe*alpha)
    
    #application du modele logit a l'echantillon de test
    predic[,i]=predict(selection,newdata=validat,type="response")
    rep=rep+predic[,i]
    cible=validat[,varY]
    pred=prediction(rep,cible,label.ordering=c(0,1))
    auc[i]=performance(pred,"auc")@y.values[[1]]
    
    #stockage des coefficients
    index=match(names(coef(selection)),names(coef(sature)))
    coef[index,i]=coef(selection)
  
  }#fin de la boucle
  
  coef[,nsimul+1]=apply(coef[,1:nsimul],1,mean)
  #moyenne des coefficients
  coef[,nsimul+2]=apply((coef[,1:nsimul]!=0),1,sum)
  
  #nb de coefficients non nuls par modalite
  
  #resultats en retour
  rep=rep/nsimul
  rf=list(auc,coef,rep)
}#fin de la fonction

seed=235
niter=5000
varx=c(varquali,varquanti)
index=match(names(credit2[id,]),varx)
varx2=varx[na.omit(index)]
bst=BoostLog(apprent=credit2[id,],validat=credit2[-id,],varY="Cible",
             varX=varx2,crit="bic",shrink=0.1,nsimul=niter)
```

```{r}
#Graphique de l'AUC
plot(bst[[1]])

max(bst[[1]])
which.max(bst[[1]])

bst[[2]][,niter+1]
```

#SVM

##SVM a noyau lineaire

```{r}
library(e1071)
svmlin=svm(Cible~.,data=credit[id,vars],kernel="linear",
           probability=TRUE,cost=1)
summary(svmlin)

tail(svmlin$coefs)

#taux de bien classes
table(svmlin$fitted,credit[id,"Cible"])
mean(svmlin$fitted!=credit[id,"Cible"])
```

```{r}
#Cross validation
svmlin=svm(Cible~.,data=credit[id,vars],kernel="linear",
           probability=TRUE,cost=1,cross=10)
summary(svmlin)

#AUC
pred=prediction(as.numeric(svmlin$fitted),credit[id,"Cible"],
                label.ordering=c(0,1))
performance(pred,"auc")@y.values[[1]]

test_svmlin=predict(svmlin,credit[-id,vars],probability=TRUE)
test_svmlin
```

```{r}
mean(test_svmlin!=credit[-id,"Cible"])
pred=prediction(attr(test_svmlin,"probabilities")[,2],credit[-id,"Cible"],
                label.ordering=c(0,1))
performance(pred,"auc")@y.values[[1]]

#Tuning cost parameter
set.seed(235)
svmlin=tune.svm(Cible~.,data=credit[id,vars],
                kernel="linear",cost=10^(-3:1))
summary(svmlin)
plot(svmlin)

#Tuning sur l'echantillon test
svmlin=tune.svm(Cible~.,data=credit[id,vars],kernel="linear",
                cost=10^(-3:1),validation.x=credit[-id,!(vars %in% c("Cible"))],
                validation.y=credit[-id,"Cible"],tunecontrol=tune.control(sampling="fix",
                                                                          fix=1))
summary(svmlin)
```

##Le calcul des coefficients des predicteurs

```{r}
x=model.matrix(~.-1,data=credit[id,1:19])
y=credit[id,20]
svmlin=svm(x,y,kernel="linear",probability=TRUE,cost=1,scale=T)
w=t(svmlin$coefs) %*% scale(x[svmlin$index,],apply(x,2,mean),apply(x,2,sd))
w

xscaled=scale(x,apply(x,2,mean),apply(x,2,sd))
p2=xscaled%*%t(w)-svmlin$rho

w_initiaux=w/apply(x,2,sd)

```

##SVM à noyau radial gaussien (RFB)

```{r}
#Tuning
library(e1071)
svmrad=tune.svm(Cible~.,data=credit[id,vars],
                kernel="radial",cost=seq(0.1,10,by=0.1),
                gamma=c(0.001,0.01,0.1,1),validation=credit[-id,!(vars %in% c("Cible"))],
                validation.y=credit[-id,"Cible"],tunecontrol=tune.control(sampling="fix",
                                                                          fix=1))
summary(svmrad)
plot(svmrad)
```

```{r}
#AUC pour des valeurs fixees de gamma et de cout
svm=svm(Cible~.,data=credit[id,vars],kernel="radial",
        gamma=0.08,cost=1.5,probability=TRUE)
svmrad=predict(svm,credit[-id,!(vars %in% c("Cible"))],probability=TRUE)
pred=prediction(attr(svmrad,"probabilities")[,2],credit[-id,"Cible"],
                label.ordering=c(0,1))
performance(pred,"auc")@y.values[[1]]
```

##SVM à noyau radial laplacien

```{r}
#Utilisation de kernlab qui dispose de plus de types de noyaux
library(kernlab)
ksvmlap=ksvm(Cible~.,data=credit[id,vars],kernel="laplacedot",
             type="C-svc",prob.model=TRUE)
ksvmlap

#Prediction
predklap=predict(ksvmlap,type="prob",credit[-id,!(vars %in% c("Cible"))])
pred=prediction(predklap[,2],credit[-id,"Cible"],label.ordering=c(0,1))
performance(pred,"auc")@y.values[[1]]
```

```{r}
#Maximisation AUC en test (pas de fonction tune pour kernlab)
maxauc=function(x){
  ksvmlap=ksvm(Cible~.,data=credit[id,vars],kernel="laplacedot",
               type="C-svc",C=x[1],prob.model=TRUE,kpar=list(sigma=x[2]))
  predklap=predict(ksvmlap,type="prob",credit[-id,!(vars %in% c("Cible"))])
  pred=prediction(predklap[,2],credit[-id,"Cible"],label.ordering = c(0,1))
  -performance(pred,"auc")@y.values[[1]]
}
algo="Nelder-Mead"
est=optim(c(1,0.05),maxauc,method=algo)
est$convergence
est$par
est$value
```

##Definition manuelle d'un noyau

```{r}
k=function(x,y){
  (sum(x*y)+1)*exp(0.001*sum((x-y)^2))
}
class(k)="kernel"
ksvmuser=ksvm(Cible~.,data=credit[id,vars],kernel=k,
              type="C-svc",C=1,prob.model=TRUE)
ksvmuser

predkuser=predict(ksvmuser,type="prob",credit[-id,!(vars %in% c("Cible"))])
pred=prediction(predkuser[,2],credit[-id,"Cible"],label.ordering = c(0,1))
performance(pred,"auc")@y.values[[1]]
```

##SVM à noyau sigmoide

```{r}
svmsig=tune.svm(Cible~.,data=credit[id,vars],
                kernel="sigmoid",cost=seq(1,100,by=0.5),gamma=c(0.0001,
                                                                0.0005,0.001,0.005,0.01,0.1,1),
                validation=credit[-id,!(vars %in% c("Cible"))],
                validation.y=credit[-id,"Cible"],tunecontrol=tune.control(sampling="fix",
                                                                          fix=1))
summary(svmsig)

svm=svm(Cible~.,data=credit[id,vars],kernel="sigmoid",
        gamma=0.01,cost=6.5,probability=TRUE)
svmsig=predict(svm,credit[-id,!(vars %in% c("Cible"))],probability=TRUE)
pred=prediction(attr(svmsig,"probabilities")[,2],credit[-id,"Cible"],
label.ordering=c(0,1))
performance(pred,"auc")@y.values[[1]]
```

##SVM à noyau polynomial

```{r}
#Noyau de degre 2
svmpol=tune.svm(Cible~.,data=credit[id,vars],kernel="polynomial",
                degree=2,gamma=seq(0.01,1,by=0.01),
                cost=seq(0.1,10,by=0.1),validation.x=credit[-id,!(vars %in% c("Cible"))],
                validation.y=credit[-id,"Cible"],tunecontrol=tune.control(sampling="fix",
                                                                          fix=1))
summary(svmpol)

svmpol$best.model

svm=svm(Cible~.,data=credit[id,vars],
        kernel="polynomial",degree=2,gamma=0.03,cost=2.9,
        probability=TRUE)
svmpol2=predict(svm,type="prob",credit[-id,!(vars %in% c("Cible"))],
                probability = TRUE)
pred=prediction(attr(svmpol2,"probabilities")[,2],credit[-id,"Cible"],label.ordering = c(0,1))
performance(pred,"auc")@y.values[[1]]
```

```{r}
#Recherche du "meilleur" coef0
library(e1071)
svmpol=tune.svm(Cible~.,data=credit[id,vars],
                kernel="polynomial",degree=2,gamma=seq(0.01,0.1,by=0.01),
                cost=seq(0.1,10,by=0.1),coef0=seq(0,1,by=0.1),validation.x=credit[-id,!(vars %in% c("Cible"))],
                validation.y=credit[-id,"Cible"],tunecontrol=tune.control(sampling="fix",fix=1))
svmpol$best.model
```

##SVM apres reduction de dimension

```{r}

#Recodage credit 2
myVariableNames=c("Comptes","Duree_credit","Historique_credit","Objet_credit","Montant_credit","Epargne","Anciennete_emploi","Taux_effort","Situation_familiale","Garanties","Anciennete_domicile","Biens","Age","Autres_credits","Statut_domicile","Nb_credits","Type_emploi","Nb_pers_charge","Telephone","Etranger","Cible")
# 
credit=read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data",h=FALSE,col.names=myVariableNames)

#credit=read.table("http://blogperso.univ-rennes1.fr/stephane.tuffery/publ#ic/german.txt",h=FALSE,col.names=myVariableNames)

#OId des observations
credit$Cle=seq(1,nrow(credit))
credit$Etranger=NULL

credit2=credit

library(car)
credit2$Comptes=recode(credit2$Comptes,"'A14'='Pas de compte';
                       'A11'='CC < 0 euros';'A12'='CC [0-200 euros[';
                       'A13'='CC > 200 euros'")

credit2$Historique_credit=recode(credit2$Historique_credit,
                                 "'A30'='Impayes passes';
                                 'A31'='Impaye en cours dans autre banque';c('A32','A33')='Pas de credits ou en cours sans retard';
                                 'A34'='Credits passes sans retard'")

credit2$Objet_credit=recode(credit2$Objet_credit,
                            "'A40'='Voiture neuve';'A41'='Voiture occasion';c('A42','A44','A45')='Interieur';'A43'='Video - HIFI';
                            c('A46','A48')='Etudes';
                            'A47'='Vacances';'A49'='Business';
                            else='Autres'")

credit2$Epargne=recode(credit2$Epargne,"
                       'A65'='Sans epargne';c('A61','A62')='< 500 euros';
                       c('A63','A64')='> 500 euros'")

credit2$Anciennete_emploi=recode(credit2$Anciennete_emploi,
                                 "c('A71','A72')='Sans emploi ou < 1 an';
                                 'A73'='entre 1 et 4 ans';
                                 c('A74','A75')='depuis au moins 4 ans'")

credit2$Situation_familiale=recode(credit2$Situation_familiale,
                                   "'A91'='Homme divorce/separe';
                                   'A92'='Femme divorcee/separee/mariee';
                                   c('A93','A94')='Homme celibataire/marie/veuf';'A95'='Femme celibataire'")

credit2$Garanties=recode(credit2$Garanties,
                         "'A103'='Avec garant';
                         else='Sans garant'")

credit2$Biens=recode(credit2$Biens,
                     "'A121'='Immobilier';
                     'A124'='Aucun bien';else='Non immobilier'")

credit2$Autres_credits=recode(credit2$Autres_credits,"'A143'='Aucun credit exterieur';
                              else='Credit exterieurs'")

credit2$Statut_domicile=recode(credit2$Statut_domicile,"'A152'='Proprietaire';
                              else='Non proprietaire'")

credit4=credit2
credit4$Taux_effort=NULL
credit4$Anciennete_domicile=NULL
credit4$Nb_credits=NULL
credit4$Nb_pers_charge=as.factor(ifelse(credit4$Nb_pers_charge==1,"NbP1","NbP2"))
credit4$Cle=NULL
credit4$Age=Age
credit4$Duree_credit=Duree_credit
credit4$Montant_credit=Montant_credit


#ACM
library(FactoMineR)
ACM=MCA(credit4,ncp=32,axes=c(1,2),graph=TRUE,quali.sup=17)
modal=apply(credit4[,-17],2,function(x) nlevels(as.factor(x)))
#Calcul du nombre de dimensions et de la somme des valeurs propres
sum(modal)-ncol(credit4[,-17])
sum(ACM$eig[,1])
sum(modal)/ncol(credit4[,-17])-1
```

```{r}
#Noyau lineaire sur les deux premiers axes factoriels
library(kernlab)
naxes=2
x=ACM$ind$coord[id,1:naxes]
y=credit[id,"Cible"]
ksvmlin=ksvm(x,y,type="C-svc",kernel="vanilladot",C=1,prob.model=TRUE)
plot(ksvmlin,data=cbind(x,y))

#Test
xt=ACM$ind$coord[-id,1:naxes]
yt=credit[-id,"Cible"]
predksvm=predict(ksvmlin,type="prob",xt)
pred=prediction(predksvm[,2],yt,label.ordering=c(1,2))
performance(pred,"auc")@y.values[[1]]
```

```{r}
#Fonction pour tester la valeur du cout
library(e1071)
f=function(i){
  ksvm=svm(x,y,kernel="linear",cost=i,probability=TRUE)
  predksvm=predict(ksvm,xt,type="prob")
  pred=prediction(predksvm,yt,label.ordering=c(1,2))
  performance(pred,"auc")@y.values[[1]]
}
f(1)

for (n in seq(2,12,by=1))
{
  i=seq(0.01,2,by=0.01)
  x=ACM$ind$coord[id,1:n]
  xt=ACM$ind$coord[-id,1:n]
  k=Vectorize(f)(i)
  cout=i[which(k==max(k),arr.ind=TRUE)[1]]
  cat("\n","nb facteur =",n," AUC test max =",max(k), "cout= ",cout)
}

```

```{r}
#Noyau raduak gaussian
f=function(i,j){
  ksvm=svm(x,y,kernel="radial",cost=i,gamma=j,probability=TRUE)
  predksvm=predict(ksvm,xt,probability=TRUE)
  pred=prediction(predksvm,yt,label.ordering = c(1,2))
  performance(pred,"auc")@y.values[[1]]
}

for (n in seq(2,12,by=1)){
  i=seq(0.05,5,by=0.05)
  j=seq(0.01,0.5,by=0.05)
  x=ACM$ind$coord[id,1:n]
  xt=ACM$ind$coord[-id,1:n]
  k=outer(i,j,Vectorize(f))
  cout=i[which(k==max(k),arr.ind=TRUE)[1,1]]
  gamma=j[which(k==max(k),arr.ind=TRUE)[1,2]]
  cat("\n","nb facteurs =",n,"AUC test max=",max(k),"cout =",cout," gamma =",gamma)
}


```

##Expression explicite du modele SVM sur composantes factorielles

```{r}
naxes=6
x=ACM$ind$coord[id,1:naxes]
y=credit[id,"Cible"]
xt=ACM$ind$coord[-id,1:naxes]
yt=credit[-id,"Cible"]
#Pas de centrage des variables
svmlin=svm(x,y,kernel="linear",probability=TRUE,cost=1,scale=F)
p1=predict(svmlin,newdata=xt,decision.values = T)
p1=attr(p1,"decision.values")
pred=prediction(p1,yt,label.ordering = c(1,2))
performance(pred,"auc")@y.values[[1]]

#Calcul des coefficients w
w=t(svmlin$coefs) %*% x[svmlin$index,]

#Extraction des coordonnees des variables de l'analyse factorielle

coef=w %*% t(ACM$var$coord[,1:naxes])
coef


```

